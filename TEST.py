#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë™ë°˜ì„±ì¥ ì§€ìˆ˜ í‰ê°€ ê´€ë ¨ ë‰´ìŠ¤ í¬ë¡¤ë§ Streamlit ì• í”Œë¦¬ì¼€ì´ì…˜
Google News RSSë¥¼ í™œìš©í•˜ì—¬ ë™ë°˜ì„±ì¥ ì§€ìˆ˜ í‰ê°€ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤.
"""

import streamlit as st
import feedparser
import requests
from bs4 import BeautifulSoup
import pandas as pd
import json
import csv
from datetime import datetime, timedelta
import time
import re
from urllib.parse import urljoin, urlparse
import plotly.express as px
import plotly.graph_objects as go
from collections import Counter
import io

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ë™ë°˜ì„±ì¥ ì§€ìˆ˜ í‰ê°€ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬",
    page_icon="ğŸ“°",
    layout="wide",
    initial_sidebar_state="expanded"
)

class NewsCrawler:
    def __init__(self):
        self.base_url = "https://news.google.com/rss"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # ë™ë°˜ì„±ì¥ ì§€ìˆ˜ ê´€ë ¨ í‚¤ì›Œë“œ
        self.keywords = [
            "ë™ë°˜ì„±ì¥ ì§€ìˆ˜",
            "ë™ë°˜ì„±ì¥ìœ„ì›íšŒ",
            "ê³µì •ê±°ë˜ìœ„ì›íšŒ",
            "ê³µì •ê±°ë˜í˜‘ì•½",
            "ì‹¤ì í‰ê°€",
            "ì´í–‰í‰ê°€",
            "ë™ë°˜ì„±ì¥ í‰ê°€",
            "ê³µì •ê±°ë˜ í‰ê°€",
            "ì¤‘ì†Œê¸°ì—… ë™ë°˜ì„±ì¥",
            "ëŒ€ê¸°ì—… ë™ë°˜ì„±ì¥",
            "ë™ë°˜ì„±ì¥ ì§€ìˆ˜ í‰ê°€",
            "ë™ë°˜ì„±ì¥ ì‹¤ì ",
            "ê³µì •ê±°ë˜ ì‹¤ì "
        ]
    
    def search_news(self, keyword, max_results=50):
        """Google News RSSì—ì„œ íŠ¹ì • í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        try:
            search_url = f"{self.base_url}/search?q={keyword}&hl=ko&gl=KR&ceid=KR:ko"
            
            # RSS í”¼ë“œ íŒŒì‹±
            feed = feedparser.parse(search_url)
            
            if not feed.entries:
                return []
            
            articles = []
            for entry in feed.entries[:max_results]:
                try:
                    article = {
                        'title': entry.get('title', ''),
                        'link': entry.get('link', ''),
                        'published': entry.get('published', ''),
                        'summary': entry.get('summary', ''),
                        'source': entry.get('source', {}).get('title', ''),
                        'keyword': keyword,
                        'crawled_at': datetime.now().isoformat()
                    }
                    
                    # URL ì •ê·œí™”
                    article['normalized_url'] = self.normalize_url(article['link'])
                    articles.append(article)
                    
                except Exception as e:
                    continue
            
            return articles
            
        except Exception as e:
            st.error(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def normalize_url(self, url):
        """URLì„ ì •ê·œí™”í•˜ì—¬ ì¤‘ë³µì„ ë°©ì§€í•©ë‹ˆë‹¤."""
        try:
            parsed = urlparse(url)
            return f"{parsed.netloc}{parsed.path}"
        except:
            return url
    
    def filter_relevant_news(self, articles):
        """ë™ë°˜ì„±ì¥ ì§€ìˆ˜ í‰ê°€ì™€ ê´€ë ¨ëœ ë‰´ìŠ¤ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤."""
        relevant_keywords = [
            "ë™ë°˜ì„±ì¥", "ê³µì •ê±°ë˜", "ì‹¤ì í‰ê°€", "ì´í–‰í‰ê°€", 
            "ë™ë°˜ì„±ì¥ìœ„ì›íšŒ", "ê³µì •ê±°ë˜ìœ„ì›íšŒ", "ê³µì •ê±°ë˜í˜‘ì•½",
            "ì¤‘ì†Œê¸°ì—…", "ëŒ€ê¸°ì—…", "ì§€ìˆ˜", "í‰ê°€"
        ]
        
        filtered_articles = []
        for article in articles:
            title = article.get('title', '').lower()
            summary = article.get('summary', '').lower()
            
            if any(keyword in title or keyword in summary for keyword in relevant_keywords):
                filtered_articles.append(article)
        
        return filtered_articles

def main():
    st.title("ğŸ“° ë™ë°˜ì„±ì¥ ì§€ìˆ˜ í‰ê°€ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬")
    st.markdown("---")
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    st.sidebar.title("âš™ï¸ ì„¤ì •")
    
    # í¬ë¡¤ë§ ì˜µì…˜
    st.sidebar.subheader("ğŸ” ê²€ìƒ‰ ì˜µì…˜")
    max_results = st.sidebar.slider("í‚¤ì›Œë“œë‹¹ ìµœëŒ€ ë‰´ìŠ¤ ìˆ˜", 10, 100, 30)
    selected_keywords = st.sidebar.multiselect(
        "ê²€ìƒ‰í•  í‚¤ì›Œë“œ ì„ íƒ",
        crawler.keywords,
        default=crawler.keywords[:5]
    )
    
    # í•„í„°ë§ ì˜µì…˜
    st.sidebar.subheader("ğŸ”§ í•„í„°ë§ ì˜µì…˜")
    filter_relevant = st.sidebar.checkbox("ê´€ë ¨ ë‰´ìŠ¤ë§Œ í‘œì‹œ", value=True)
    remove_duplicates = st.sidebar.checkbox("ì¤‘ë³µ ì œê±°", value=True)
    
    # ë©”ì¸ ì»¨í…ì¸ 
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("ğŸš€ ë‰´ìŠ¤ í¬ë¡¤ë§")
        
        if st.button("ğŸ“¡ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘", type="primary"):
            if not selected_keywords:
                st.warning("ê²€ìƒ‰í•  í‚¤ì›Œë“œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
                return
            
            # ì§„í–‰ë°”
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            all_articles = []
            seen_urls = set()
            
            for i, keyword in enumerate(selected_keywords):
                status_text.text(f"ê²€ìƒ‰ ì¤‘: {keyword}")
                progress_bar.progress((i + 1) / len(selected_keywords))
                
                articles = crawler.search_news(keyword, max_results)
                
                for article in articles:
                    if remove_duplicates and article['normalized_url'] in seen_urls:
                        continue
                    if remove_duplicates:
                        seen_urls.add(article['normalized_url'])
                    all_articles.append(article)
                
                time.sleep(1)  # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
            
            # í•„í„°ë§
            if filter_relevant:
                all_articles = crawler.filter_relevant_news(all_articles)
            
            # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
            st.session_state['news_data'] = all_articles
            st.session_state['crawl_time'] = datetime.now()
            
            status_text.text("âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
            progress_bar.progress(1.0)
            
            st.success(f"ì´ {len(all_articles)}ê°œì˜ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
    
    with col2:
        st.subheader("ğŸ“Š í†µê³„")
        
        if 'news_data' in st.session_state:
            news_data = st.session_state['news_data']
            
            # ê¸°ë³¸ í†µê³„
            st.metric("ì´ ë‰´ìŠ¤ ìˆ˜", len(news_data))
            
            # í‚¤ì›Œë“œë³„ í†µê³„
            keyword_counts = Counter([article['keyword'] for article in news_data])
            st.write("**í‚¤ì›Œë“œë³„ ë‰´ìŠ¤ ìˆ˜:**")
            for keyword, count in keyword_counts.most_common(5):
                st.write(f"â€¢ {keyword}: {count}ê°œ")
            
            # ì¶œì²˜ë³„ í†µê³„
            source_counts = Counter([article['source'] for article in news_data if article['source']])
            st.write("**ì£¼ìš” ì¶œì²˜:**")
            for source, count in source_counts.most_common(3):
                st.write(f"â€¢ {source}: {count}ê°œ")
    
    # ë‰´ìŠ¤ ë°ì´í„° í‘œì‹œ
    if 'news_data' in st.session_state and st.session_state['news_data']:
        st.markdown("---")
        st.subheader("ğŸ“° ìˆ˜ì§‘ëœ ë‰´ìŠ¤")
        
        # ê²€ìƒ‰ ë° í•„í„°ë§
        search_term = st.text_input("ğŸ” ë‰´ìŠ¤ ê²€ìƒ‰:", placeholder="ì œëª©ì´ë‚˜ ë‚´ìš©ìœ¼ë¡œ ê²€ìƒ‰...")
        
        col_filter1, col_filter2, col_filter3 = st.columns(3)
        
        with col_filter1:
            keyword_filter = st.selectbox("í‚¤ì›Œë“œ í•„í„°:", ["ì „ì²´"] + list(set([article['keyword'] for article in st.session_state['news_data']])))
        
        with col_filter2:
            source_filter = st.selectbox("ì¶œì²˜ í•„í„°:", ["ì „ì²´"] + list(set([article['source'] for article in st.session_state['news_data'] if article['source']])))
        
        with col_filter3:
            sort_option = st.selectbox("ì •ë ¬:", ["ìµœì‹ ìˆœ", "ì œëª©ìˆœ", "ì¶œì²˜ìˆœ"])
        
        # ë°ì´í„° í•„í„°ë§
        filtered_data = st.session_state['news_data'].copy()
        
        if search_term:
            filtered_data = [article for article in filtered_data 
                           if search_term.lower() in article['title'].lower() or 
                           search_term.lower() in article.get('summary', '').lower()]
        
        if keyword_filter != "ì „ì²´":
            filtered_data = [article for article in filtered_data if article['keyword'] == keyword_filter]
        
        if source_filter != "ì „ì²´":
            filtered_data = [article for article in filtered_data if article['source'] == source_filter]
        
        # ì •ë ¬
        if sort_option == "ìµœì‹ ìˆœ":
            filtered_data.sort(key=lambda x: x.get('published', ''), reverse=True)
        elif sort_option == "ì œëª©ìˆœ":
            filtered_data.sort(key=lambda x: x.get('title', ''))
        elif sort_option == "ì¶œì²˜ìˆœ":
            filtered_data.sort(key=lambda x: x.get('source', ''))
        
        # í˜ì´ì§€ë„¤ì´ì…˜
        items_per_page = 10
        total_pages = (len(filtered_data) + items_per_page - 1) // items_per_page
        current_page = st.selectbox("í˜ì´ì§€:", range(1, total_pages + 1), index=0)
        
        start_idx = (current_page - 1) * items_per_page
        end_idx = start_idx + items_per_page
        page_data = filtered_data[start_idx:end_idx]
        
        # ë‰´ìŠ¤ ì¹´ë“œ í‘œì‹œ
        for i, article in enumerate(page_data):
            with st.expander(f"ğŸ“° {article['title'][:80]}..."):
                col_info, col_link = st.columns([3, 1])
                
                with col_info:
                    st.write(f"**ì¶œì²˜:** {article['source']}")
                    st.write(f"**í‚¤ì›Œë“œ:** {article['keyword']}")
                    st.write(f"**ë°œí–‰ì¼:** {article['published']}")
                    
                    if article.get('summary'):
                        st.write(f"**ìš”ì•½:** {article['summary'][:200]}...")
                
                with col_link:
                    if st.button(f"ğŸ”— ë§í¬", key=f"link_{i}"):
                        st.write(f"[ê¸°ì‚¬ ë³´ê¸°]({article['link']})")
        
        # ë°ì´í„° ë‚´ë³´ë‚´ê¸°
        st.markdown("---")
        st.subheader("ğŸ’¾ ë°ì´í„° ë‚´ë³´ë‚´ê¸°")
        
        col_export1, col_export2, col_export3 = st.columns(3)
        
        with col_export1:
            if st.button("ğŸ“„ JSON ë‹¤ìš´ë¡œë“œ"):
                json_data = json.dumps(filtered_data, ensure_ascii=False, indent=2)
                st.download_button(
                    label="JSON íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                    data=json_data,
                    file_name=f"ë™ë°˜ì„±ì¥_ë‰´ìŠ¤_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                    mime="application/json"
                )
        
        with col_export2:
            if st.button("ğŸ“Š CSV ë‹¤ìš´ë¡œë“œ"):
                df = pd.DataFrame(filtered_data)
                csv_data = df.to_csv(index=False, encoding='utf-8-sig')
                st.download_button(
                    label="CSV íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                    data=csv_data,
                    file_name=f"ë™ë°˜ì„±ì¥_ë‰´ìŠ¤_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                    mime="text/csv"
                )
        
        with col_export3:
            if st.button("ğŸ“ˆ Excel ë‹¤ìš´ë¡œë“œ"):
                df = pd.DataFrame(filtered_data)
                excel_buffer = io.BytesIO()
                df.to_excel(excel_buffer, index=False, engine='openpyxl')
                st.download_button(
                    label="Excel íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                    data=excel_buffer.getvalue(),
                    file_name=f"ë™ë°˜ì„±ì¥_ë‰´ìŠ¤_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )
    
    # ì‹œê°í™”
    if 'news_data' in st.session_state and st.session_state['news_data']:
        st.markdown("---")
        st.subheader("ğŸ“Š ë°ì´í„° ì‹œê°í™”")
        
        news_data = st.session_state['news_data']
        df = pd.DataFrame(news_data)
        
        col_viz1, col_viz2 = st.columns(2)
        
        with col_viz1:
            # í‚¤ì›Œë“œë³„ ë‰´ìŠ¤ ìˆ˜ ì°¨íŠ¸
            keyword_counts = df['keyword'].value_counts()
            fig1 = px.bar(
                x=keyword_counts.index, 
                y=keyword_counts.values,
                title="í‚¤ì›Œë“œë³„ ë‰´ìŠ¤ ìˆ˜",
                labels={'x': 'í‚¤ì›Œë“œ', 'y': 'ë‰´ìŠ¤ ìˆ˜'}
            )
            fig1.update_xaxis(tickangle=45)
            st.plotly_chart(fig1, use_container_width=True)
        
        with col_viz2:
            # ì¶œì²˜ë³„ ë‰´ìŠ¤ ìˆ˜ ì°¨íŠ¸
            source_counts = df['source'].value_counts().head(10)
            fig2 = px.pie(
                values=source_counts.values,
                names=source_counts.index,
                title="ì£¼ìš” ì¶œì²˜ë³„ ë‰´ìŠ¤ ë¹„ìœ¨"
            )
            st.plotly_chart(fig2, use_container_width=True)
        
        # ì‹œê°„ë³„ ë‰´ìŠ¤ íŠ¸ë Œë“œ
        if len(news_data) > 0:
            st.subheader("ğŸ“ˆ ì‹œê°„ë³„ ë‰´ìŠ¤ íŠ¸ë Œë“œ")
            
            # ë°œí–‰ì¼ íŒŒì‹± ë° ê·¸ë£¹í™”
            df['published_date'] = pd.to_datetime(df['published'], errors='coerce')
            df['date'] = df['published_date'].dt.date
            
            daily_counts = df.groupby('date').size().reset_index(name='count')
            
            if not daily_counts.empty:
                fig3 = px.line(
                    daily_counts, 
                    x='date', 
                    y='count',
                    title="ì¼ë³„ ë‰´ìŠ¤ ìˆ˜ íŠ¸ë Œë“œ"
                )
                st.plotly_chart(fig3, use_container_width=True)

# ì „ì—­ ë³€ìˆ˜
crawler = NewsCrawler()

if __name__ == "__main__":
    main()
